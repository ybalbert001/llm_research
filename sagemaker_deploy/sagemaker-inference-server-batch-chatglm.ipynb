{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ae65fec",
   "metadata": {},
   "source": [
    "# Deploy the chatglm model on Amazon SageMaker\n",
    "\n",
    "As we have finetuned the model, next we will show you how to deploy the model on SageMaker.\n",
    "\n",
    "In this notebook, we explore how to host a large language model on SageMaker using the [Large Model Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-inference.html) container that is optimized for hosting large models using DJLServing. DJLServing is a high-performance universal model serving solution powered by the Deep Java Library (DJL) that is programming language agnostic. To learn more about DJL and DJLServing, you can refer to our recent [blog post](https://aws.amazon.com/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2407531",
   "metadata": {},
   "source": [
    "## Create a SageMaker Model for Deployment\n",
    "As a first step, we'll import the relevant libraries and configure several global variables such as the hosting image that will be used nd the S3 location of our model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5806a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import serializers, deserializers\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import jinja2\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24862c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "region = sess._region_name # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "\n",
    "s3_client = boto3.client(\"s3\") # client to intreract with S3 API\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\") # client to intreract with SageMaker Endpoints\n",
    "jinja_env = jinja2.Environment() # jinja environment to generate model configuration templates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d3df933-a7a3-4b70-8806-3115c68bb22a",
   "metadata": {},
   "source": [
    "如下镜像可以换新的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38930529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup the inference image uri based on our current region\n",
    "djl_inference_image_uri = (\n",
    "    f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.22.1-deepspeed0.9.2-cu118\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7d677d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "local_cache_path = Path(\"./model\")\n",
    "local_cache_path.mkdir(exist_ok=True)\n",
    "\n",
    "model_name = \"THUDM/chatglm-6b\"# 下载模型，如果模型已经在 s3了，可以跳过 直接到 deploy\n",
    "\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.model\", \"*.py\"]\n",
    "\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_cache_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ef8ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model files path\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "local_model_path = None\n",
    "\n",
    "paths = os.walk(r'./model')\n",
    "for root, dirs, files in paths:\n",
    "    for file in files:\n",
    "        if file == 'config.json':\n",
    "            print(os.path.join(root,file))\n",
    "            local_model_path = str(os.path.join(root,file))[0:-11]\n",
    "            print(local_model_path)\n",
    "if local_model_path == None:\n",
    "    print(\"Model download may failed, please check prior step!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b45a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script env sagemaker_default_bucket=$bucket local_model_path=$local_model_path bash\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync ${local_model_path} s3://${sagemaker_default_bucket}/chatglm-6b/\n",
    "\n",
    "rm -rf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00f2f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_location = f\"s3://{bucket}/chatglm-6b/\"# Change to the model artifact path in S3 \n",
    "print(f\"Pretrained model will be downloaded from ---- > {pretrained_model_location}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d453baa9",
   "metadata": {},
   "source": [
    "## Let's try DeepSpeed for LMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f6e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model(image_uri, model_data, role, endpoint_name, instance_type, sagemaker_session):\n",
    "    \n",
    "    \"\"\"Helper function to create the SageMaker Endpoint resources and return a predictor\"\"\"\n",
    "    model = Model(\n",
    "            image_uri=image_uri, \n",
    "              model_data=model_data, \n",
    "              role=role\n",
    "             )\n",
    "    \n",
    "    model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=instance_type,\n",
    "        endpoint_name=endpoint_name,\n",
    "        container_startup_health_check_timeout=60*10\n",
    "        )\n",
    "    \n",
    "    # our requests and responses will be in json format so we specify the serializer and the deserializer\n",
    "    predictor = sagemaker.Predictor(\n",
    "        endpoint_name=endpoint_name, \n",
    "        sagemaker_session=sagemaker_session, \n",
    "        serializer=serializers.JSONSerializer(), \n",
    "        deserializer=deserializers.JSONDeserializer())\n",
    "    \n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9605d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf deepspeed_src\n",
    "!mkdir deepspeed_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448335bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile deepspeed_src/serving.template\n",
    "engine=DeepSpeed\n",
    "option.s3url={{ s3url }}\n",
    "option.tensor_parallel_degree=1\n",
    "batch_size=4\n",
    "max_batch_delay=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0346af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we plug in the appropriate model location into our `serving.properties` file based on the region in which this notebook is running\n",
    "template = jinja_env.from_string(Path(\"deepspeed_src/serving.template\").open().read())\n",
    "Path(\"deepspeed_src/serving.properties\").open(\"w\").write(template.render(s3url=pretrained_model_location))\n",
    "!pygmentize deepspeed_src/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a77b099",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile deepspeed_src/model.py\n",
    "from djl_python import Input, Output\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import deepspeed\n",
    "import transformers\n",
    "# from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "import re\n",
    "\n",
    "model = None\n",
    "#here, we need to set the global variable batch_size according to the batch_size in the serving.properties file.\n",
    "batch_size = 4\n",
    "\n",
    "def process_response(response):\n",
    "    response = response.strip()\n",
    "    response = response.replace(\"[[训练时间]]\", \"2023年\")\n",
    "    punkts = [\n",
    "        [\",\", \"，\"],\n",
    "        [\"!\", \"！\"],\n",
    "        [\":\", \"：\"],\n",
    "        [\";\", \"；\"],\n",
    "        [\"\\?\", \"？\"],\n",
    "    ]\n",
    "    for item in punkts:\n",
    "        response = re.sub(r\"([\\u4e00-\\u9fff])%s\" % item[0], r\"\\1%s\" % item[1], response)\n",
    "        response = re.sub(r\"%s([\\u4e00-\\u9fff])\" % item[0], r\"%s\\1\" % item[1], response)\n",
    "    return response\n",
    "\n",
    "def load_model(properties):\n",
    "\n",
    "    model_location = properties['model_dir']\n",
    "    tensor_parallel = properties[\"tensor_parallel_degree\"]\n",
    "    \n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties['model_id']\n",
    "    logging.info(f\"Loading model in {model_location}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_location, trust_remote_code=True)\n",
    "\n",
    "    model = AutoModel.from_pretrained(model_location, low_cpu_mem_usage=True, torch_dtype=torch.float16, trust_remote_code=True)\n",
    "    print(\"----------model dtype is {0}---------\".format(model.dtype))\n",
    "    \n",
    "    model = deepspeed.init_inference(\n",
    "        model,\n",
    "        mp_size=tensor_parallel,\n",
    "        dtype=torch.half,\n",
    "        replace_method=\"auto\",\n",
    "        replace_with_kernel_inject=True,\n",
    "    )\n",
    "    \n",
    "#     local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "#     generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, use_cache=True, device=local_rank)\n",
    "    \n",
    "    model.requires_grad_(False)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def handle(inputs: Input) -> None:\n",
    "\n",
    "    global model, tokenizer\n",
    "\n",
    "    try:\n",
    "        if not model:\n",
    "            model,tokenizer = load_model(inputs.get_properties())\n",
    "\n",
    "        if inputs.is_empty():\n",
    "            # Model server makes an empty call to warmup the model on startup\n",
    "            return None\n",
    "        \n",
    "        if inputs.is_batch():\n",
    "            #the demo code is just suitable for single sample per client request\n",
    "            bs = inputs.get_batch_size()\n",
    "            logging.info(f\"Dynamic batching size: {bs}.\")\n",
    "            batch = inputs.get_batches()\n",
    "            #print(batch)\n",
    "            tmp_inputs = []\n",
    "            for _, item in enumerate(batch):\n",
    "                tmp_item = item.get_as_json()\n",
    "                tmp_inputs.append(tmp_item.get(\"input\"))\n",
    "            \n",
    "            #For server side batch, we just use the custom generation parameters for single Sagemaker Endpoint.\n",
    "            inputs = tokenizer([tmp_inputs], return_tensors=\"pt\")\n",
    "            inputs = inputs.to('cuda')\n",
    "            gen_kwargs = {\"max_length\": 128, \"do_sample\": True,\n",
    "                      \"temperature\": 1.0}\n",
    "            result = model.generate(**inputs, **gen_kwargs)\n",
    "#             result = predictor(tmp_inputs, batch_size = bs, max_new_tokens = 128, min_new_tokens = 128, temperature = 1.0, do_sample = True)\n",
    "            result = result.tolist()[0][len(inputs[\"input_ids\"][0]):]\n",
    "            result = tokenizer.decode(result)\n",
    "            result = process_response(result)\n",
    "            \n",
    "            outputs = Output()\n",
    "            for i in range(len(result)):\n",
    "                outputs.add(result[i], key=\"generate_text\", batch_index=i)\n",
    "            return outputs\n",
    "        else:\n",
    "            inputs = inputs.get_as_json()\n",
    "            if not inputs.get(\"input\"):\n",
    "                return Output().add_as_json({\"code\":-1,\"msg\":\"input field can't be null\"})\n",
    "\n",
    "            #input data\n",
    "            data = inputs.get(\"input\")\n",
    "            params = inputs.get(\"params\",{})\n",
    "\n",
    "            #for pure client side batch\n",
    "            if type(data) == str:\n",
    "                bs = 1\n",
    "            elif type(data) == list:\n",
    "                if len(data) > batch_size:\n",
    "                    bs = batch_size\n",
    "                else:\n",
    "                    bs = len(data)\n",
    "            else:\n",
    "                return Output().add_as_json({\"code\":-1,\"msg\": \"input has wrong type\"})\n",
    "                \n",
    "            print(\"client side batch size is \", bs)\n",
    "            #predictor\n",
    "#             result = predictor(data, batch_size = bs, **params)\n",
    "#             result = model.chat(tokenizer, data, **params)\n",
    "            inputs = tokenizer(data, return_tensors=\"pt\")\n",
    "            inputs = inputs.to('cuda')\n",
    "            gen_kwargs = {\"max_length\": 50, \"do_sample\": True,\n",
    "                      \"temperature\": 0.5}\n",
    "            result = model.generate(**inputs, **gen_kwargs)\n",
    "            result = result.tolist()[0][len(inputs[\"input_ids\"][0]):]\n",
    "            result = tokenizer.decode(result)\n",
    "            result = process_response(result)\n",
    "            #return\n",
    "            return Output().add({\"code\":0,\"msg\":\"ok\",\"data\":result})\n",
    "    except Exception as e:\n",
    "        return Output().add_as_json({\"code\":-1,\"msg\":e})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b2654",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar czvf ds_model.tar.gz deepspeed_src/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4945c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_code_prefix = \"chatglm/deploy/code\"\n",
    "\n",
    "code_artifact = sess.upload_data(\"ds_model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3eea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a unique endpoint name\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"chatglm-6B-ds\")\n",
    "print(f\"Our endpoint will be called {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26e0b1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# deployment will take about 10 minutes\n",
    "ds_predictor = deploy_model(image_uri=djl_inference_image_uri, \n",
    "                            model_data=code_artifact, \n",
    "                            role=role, \n",
    "                            endpoint_name=endpoint_name, \n",
    "                            instance_type=\"ml.g4dn.xlarge\", \n",
    "                            sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67aedbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_predictor.predict({ \n",
    "                    \"input\" : \"请介绍下你自己\", \n",
    "#                     \"params\": { \"max_length\": 50, \"temperature\": 0.5}\n",
    "                })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8a372c4",
   "metadata": {},
   "source": [
    "## Clear resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4843a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6a6af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81a87ae7",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1a842b9",
   "metadata": {},
   "source": [
    "[sagemaker-hosting/Large-Language-Model-Hosting/](https://github.com/aws-samples/sagemaker-hosting/tree/main/Large-Language-Model-Hosting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
